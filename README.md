# üõ†Ô∏è Data Engineer Portfolio

Hi there! I'm Anna ‚Äî welcome to my growing **Data Engineering Portfolio** üöÄ

This repository showcases a collection of hands-on projects I've completed through various data analytics and engineering courses, as well as personal learning challenges. Each project is designed to strengthen key skills in the modern data stack ‚Äî from data wrangling and pipeline building to database management and cloud integration.


### üí° What you‚Äôll find here:

- Projects using Python, pandas, SQL, dbt and Git  
- ETL workflows and data pipeline simulations  
- Practice with cloud tools like AWS and GCP  
- Data modeling and visualization examples  
- Experiments and self-driven exercises for continuous learning  


# [Web Scrapping - Beautiful Soup](https://github.com/AnnaPrus/web_scrapping/tree/main)

- Project Overview: Used Webscraping techniques to extract information from the website "the list of the top 10 largest banks in the world" as per requirement. Used Pandas data frames and dictionaries to transform data as per requirement. Loaded the processed information to CSV files and as Database tables. Queried the database tables using SQLite3 and pandas libraries. Logged the progress of the code properly.

- Technologies Used:
  - Python: Primary programming language
  - Beautiful Soup: Web scraping and HTML parsing
  - Pandas:Data manipulation and analysis

- Final Outcome: *[the output file with the list of trasformed banks](https://github.com/AnnaPrus/web_scrapping/blob/main/banks_transformed.csv)*

# [Data Analysis Using PySpark](https://github.com/AnnaPrus/data_analysis_using_spark)
- Project Overview: This project demonstrates hands-on data analysis using PySpark, the Python API for Apache Spark. The primary objective is to showcase practical data engineering and advanced analytics skills by performing a series of structured transformations and queries on employee data.

- Key goals include:

  - Loading and structuring data efficiently from a CSV file.
  - Developing a custom schema for accurate data representation.
  - Applying SQL queries and DataFrame operations for filtering, aggregation, and calculation.
  - Solving business-like analytics questions, such as department statistics, salary computations, and employee segmentation.
  - Practicing code modularity and data-focused programming within a distributed computing framework.

- Technologies used: 
  - PySpark: the Python API for Apache Spark, a fast, distributed engine for big data processing and analytics.
  - Python 3.x: Main programming language for code implementation.
  - Jupyter Notebook/IPython: Interactive environment for development and results visualization.
  - Spark SQL: SQL interface for querying structured data within Spark.
  - DataFrame API: For columnar data processing, transformations, and aggregations.
  - findspark: Integration utility for Spark in Python environments.
 
# [M&M Color Count by US State using Apache Spark](https://github.com/AnnaPrus/count_mnm_with_spark)  

- Project Overview: this project processes a CSV dataset of M&M candy counts by color across different US states. Using Apache Spark and PySpark, it reads the data, aggregates counts by color and state, and outputs the summarized results.

count_mnm_with_spark/
‚îú‚îÄ‚îÄ mnm_data.csv
‚îú‚îÄ‚îÄ count_mnm.py
‚îî‚îÄ‚îÄ README.md


<hr style="border: 0,5px solid #ccc;">

[My Linkedin Profile](https://www.linkedin.com/in/anna-prus-solutions-engineer/)


